# Implementation Plan Document: RAG-based Conversational AI Chatbot for Restaurant Data

This document provides a step-by-step guide to implement a conversational AI chatbot using a Retrieval-Augmented Generation (RAG) pipeline. The chatbot will answer queries about restaurants, menus, and ingredients by integrating structured CSV data with unstructured external data and by using OpenAI's GPT-3.5-Turbo for generating responses. Each step includes checkboxes for you to mark off as you complete it.

## Table of Contents

1. [Environment Setup](#environment-setup) âœ…
2. [Project Structure Setup](#project-structure-setup) âœ…
3. [Data Ingestion Pipeline](#data-ingestion-pipeline) âœ…
4. [Text Chunking and Embedding Generation](#text-chunking-and-embedding-generation) âœ…
5. [Storing Embeddings in the Vector Database](#storing-embeddings-in-the-vector-database) âœ…
6. [Query Processing Pipeline](#query-processing-pipeline) âœ…
7. [LLM Integration and Prompt Engineering](#llm-integration-and-prompt-engineering) âœ…
8. [Testing and Debugging](#testing-and-debugging) âœ…
9. [Frontend Development](#frontend-development) ðŸš§
10. [Deployment](#deployment) ðŸš§
11. [Final Checklist](#final-checklist) ðŸš§

## Latest Updates (2025-02-04)

### Completed Features âœ…
1. Fixed conversation serialization issues
2. Implemented proper rate limiting with custom retry times
3. Added comprehensive test coverage for API endpoints
4. Enhanced error handling and validation
5. Improved conversation management with persistence
6. Added proper metadata handling for conversations

### Remaining Tasks ðŸš§
1. Frontend Development
   - Design and implement the chat interface
   - Add restaurant search functionality
   - Implement conversation history view
   - Add rate limit indicators
   - Create loading states and error handling

2. Deployment
   - Set up CI/CD pipeline
   - Configure production environment
   - Set up monitoring and logging
   - Implement backup strategy
   - Document deployment process

3. Documentation
   - Complete API documentation
   - Add deployment guide
   - Create user manual
   - Document frontend components
   - Add troubleshooting guide

## Environment Setup âœ…

### 1.1 Install Python and Create a Virtual Environment âœ…

- [x] Install Python 3.8+ on your machine (Completed: 2025-02-03)
- [x] Create a virtual environment by running:
  ```bash
python3 -m venv env
  ```
- [x] Activate the virtual environment:
  ```bash
  # On Windows:
  .\env\Scripts\activate
  
  # On macOS/Linux:
  source env/bin/activate
  ```

### 1.2 Install Required Packages âœ…

- [x] Create a file named "requirements.txt" with the following lines:
  ```
  openai==1.3.0
  pinecone-client==2.2.4
  faiss-cpu==1.7.4
  pandas==2.1.4
  numpy==1.26.2
  requests==2.31.0
  tqdm==4.66.1
  python-dotenv==1.0.0
  ```
- [x] Install the dependencies by running:
  ```bash
pip install -r requirements.txt
  ```
- [x] Verify the installation with:
  ```bash
pip list
  ```

## Project Structure Setup âœ…

### 2.1 Create the Directory Structure âœ…

- [x] Create the following structure:
  ```
  project/
  â”œâ”€â”€ data/
  â”‚   â””â”€â”€ sample_restaurant_data.csv
  â”œâ”€â”€ src/
  â”‚   â”œâ”€â”€ ingestion.py
  â”‚   â”œâ”€â”€ chunking.py
  â”‚   â”œâ”€â”€ embedding.py
  â”‚   â”œâ”€â”€ vector_db.py
  â”‚   â”œâ”€â”€ query.py
  â”‚   â””â”€â”€ main.py
  â”œâ”€â”€ requirements.txt
  â””â”€â”€ README.md
  ```

### 2.2 Initialize Git Repository âœ…

- [x] Initialize Git in your project folder:
  ```bash
git init
  ```
- [x] Create a .gitignore file to exclude your virtual environment folder and other temporary files.

## Data Ingestion Pipeline âœ…

### 3.1 Read the CSV File âœ…

- [x] In "src/ingestion.py", write code to load the CSV file using Pandas:
  ```python
import pandas as pd

  def load_csv(file_path):
      return pd.read_csv(file_path)

  if __name__ == "__main__":
      df = load_csv('../data/sample_restaurant_data.csv')
      print(df.head())
  ```
- [x] Test by running:
  ```bash
python src/ingestion.py
  ```

### 3.2 Fetch Unstructured Data (Optional) âœ…

- [x] In "src/ingestion.py", add a function to fetch data from an external source:
  ```python
import requests

  def fetch_wikipedia_article(title):
      url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{title}"
      response = requests.get(url)
      if response.status_code == 200:
          return response.json().get("extract", "")
      return ""
  ```
- [x] Test the function with a sample article title (Completed: Successfully fetched and enriched data with cuisine descriptions)

## Text Chunking and Embedding Generation âœ…

### 4.1 Implement Text Chunking âœ…

- [x] In "src/chunking.py", write functions to split text into manageable chunks:
  ```python
  @dataclass
  class TextChunk:
      text: str
      metadata: Dict[str, Any]
      token_count: int

  def chunk_text(text: str, max_tokens: int = 500) -> List[str]:
      # Implementation handles:
      # - Sentence boundaries
      # - Long sentence splitting
      # - Token count estimation
      # - Metadata preservation
      pass

  def create_restaurant_chunks(restaurant_data: Dict[str, Any]) -> List[TextChunk]:
      # Creates chunks for restaurant data with metadata
      pass

  def get_ingredient_chunks(ingredients: List[str]) -> List[TextChunk]:
      # Creates chunks for ingredient lists
      pass
  ```
- [x] Test the chunking functions with sample text (Completed: Successfully processed 52,696 records into 10,716 chunks)

### 4.2 Generate Embeddings Using OpenAI âœ…

- [x] In "src/embedding.py", implement a function to generate embeddings:
  ```python
  def get_embedding(text, model="text-embedding-ada-002"):
      response = client.embeddings.create(
          input=[text],
          model=model
      )
      return response.data[0].embedding
  ```
- [x] Test the embedding function with sample text snippets (Completed: Successfully generated embeddings for restaurant data chunks)
- [x] Implement batch processing for multiple chunks
- [x] Add save/load functionality for embeddings
- [x] Add proper error handling and retries
- [x] Set up secure API key management

## Storing Embeddings in the Vector Database âœ…

### 5.1 Set Up Pinecone (Using Free Tier) âœ…

- [x] Sign up for Pinecone and obtain your API key
- [x] In "src/vector_db.py", write functions to initialize and interact with Pinecone:
  ```python
  def init_pinecone():
      pc = Pinecone(api_key=os.getenv('PINECONE_API_KEY'))
      return pc.Index(INDEX_NAME)

  def upsert_embeddings(index, embedded_chunks):
      # Prepare vectors with IDs and metadata
      vectors = []
      for i, chunk in enumerate(embedded_chunks):
          vectors.append({
              'id': f"chunk_{i}",
              'values': chunk.embedding,
              'metadata': chunk.metadata
          })
      
      # Upsert in batches
      batch_size = 100
      for i in range(0, len(vectors), batch_size):
          batch = vectors[i:i + batch_size]
          index.upsert(vectors=batch)
  ```
- [x] Test the connection and basic operations (Completed: Successfully connected and verified index)
- [x] Implement batch upserting for efficiency (Completed: Implemented with batch size of 100)
- [x] Add error handling and retries (Completed: Added retry logic with max 3 attempts)
- [x] Verify data persistence (Completed: Successfully queried back upserted vectors)

## Query Processing Pipeline âœ…

### 6.1 Embed User Queries âœ…

- [x] In "src/query.py", implemented code to embed user queries:
  ```python
  def embed_query(query: str) -> Optional[List[float]]:
      try:
          # Clean and validate query
          if not query or not query.strip():
              print("Error: Query cannot be empty")
              return None
              
          # Generate embedding
          embedding = get_embedding(query.strip())
          if embedding:
              return convert_to_native_types(embedding)
          return None
          
      except Exception as e:
          print(f"Error embedding query: {str(e)}")
          return None
  ```
- [x] Added comprehensive tests for query embedding
- [x] Implemented error handling and validation
- [x] Added type hints and documentation

### 6.2 Retrieve Similar Context from the Vector DB âœ…

- [x] In "src/vector_db.py", implemented vector search functionality:
  ```python
  def query_similar(
      index: pinecone.Index,
      query_embedding: List[float],
      top_k: int = 5,
      score_threshold: float = 0.7,
      filter: Optional[Dict] = None
  ) -> List[Dict[str, Any]]:
      try:
          results = index.query(
              vector=query_embedding,
              top_k=top_k,
              include_metadata=True,
              filter=filter
          )
          
          filtered_results = []
          for match in results.matches:
              if match.score >= score_threshold:
                  filtered_results.append({
                      "id": match.id,
                      "score": match.score,
                      "metadata": match.metadata
                  })
          
          return filtered_results
          
      except Exception as e:
          print(f"Error querying similar vectors: {str(e)}")
          return []
  ```
- [x] Added comprehensive tests for vector search
- [x] Implemented filtering and score thresholds
- [x] Added proper error handling
- [x] Added type hints and documentation

## LLM Integration and Prompt Engineering âœ…

### 7.1 Construct the Prompt âœ…

- [x] In "src/chat.py", implemented prompt construction with context:
  ```python
  system_prompt = (
      "You are a helpful assistant for a restaurant information system. "
      "Use the provided context to answer questions about restaurants, "
      "their menus, and related information. If you're not sure about "
      "something, say so rather than making assumptions."
  )
  
  user_prompt = f"Question: {query}\n\nRelevant Context:\n{context_text}"
  ```
- [x] Added conversation history management
- [x] Implemented context retrieval and integration
- [x] Added proper formatting and structure

### 7.2 Integrate with OpenAI GPT-3.5-Turbo âœ…

- [x] In "src/chat.py", implemented response generation:
  ```python
  def generate_response(
      query: str,
      conversation_history: ConversationHistory,
      client: OpenAI,
      get_similar_chunks: Callable,
      max_tokens: int = 500,
      temperature: float = 0.7
  ) -> Optional[str]:
      try:
          # Get relevant context from vector search
          context_chunks = get_similar_chunks(query, top_k=3)
          
          # Generate response
          response = client.chat.completions.create(
              model="gpt-3.5-turbo",
              messages=messages,
              max_tokens=max_tokens,
              temperature=temperature,
              n=1,
              stop=None
          )
          
          return response.choices[0].message.content.strip()
          
      except Exception as e:
          print(f"Error generating response: {str(e)}")
          return None
  ```
- [x] Added comprehensive tests for response generation
- [x] Implemented error handling and retries
- [x] Added proper configuration options
- [x] Added type hints and documentation

## Testing and Debugging âœ…

### 8.1 Implement End-to-End Tests âœ…

- [x] Created comprehensive test suite in `tests/test_chat_e2e.py`:
  ```python
  def test_complete_chat_flow()  # Tests full chat interaction flow
  def test_conversation_persistence()  # Tests conversation storage
  def test_conversation_cleanup()  # Tests cleanup of old conversations
  def test_error_scenarios()  # Tests error handling
  def test_context_window_handling()  # Tests conversation context management
  ```
- [x] Implemented mock fixtures for external dependencies:
  - [x] Mock OpenAI client
  - [x] Mock Pinecone client
  - [x] Mock vector search function
  - [x] Mock rate limiting
- [x] Added test storage directory fixture for conversation persistence
- [x] Verified all tests passing successfully

### 8.2 Fix Critical Issues âœ…

- [x] Fixed conversation storage directory handling:
  - [x] Implemented proper storage_dir property in ConversationManager
  - [x] Added automatic directory creation
  - [x] Ensured correct path resolution
  - [x] Updated all conversations when storage directory changes

- [x] Fixed context window handling:
  - [x] Eliminated message duplication in chat responses
  - [x] Improved vector search mock to prevent duplicates
  - [x] Verified correct message count in context window

- [x] Enhanced conversation cleanup:
  - [x] Fixed timestamp handling for both files and objects
  - [x] Improved cleanup logic for old conversations
  - [x] Added proper file deletion

### 8.3 Code Quality Improvements âœ…

- [x] Added comprehensive error handling
- [x] Improved logging and debugging messages
- [x] Enhanced type hints and documentation
- [x] Implemented proper API error responses
- [x] Added rate limiting with configurable thresholds

## Frontend Development ðŸš§

### 9.1 Design and Implement Chat Interface ðŸš§

- [ ] Design the chat interface
- [ ] Implement chat functionality
- [ ] Add restaurant search functionality
- [ ] Implement conversation history view
- [ ] Add rate limit indicators
- [ ] Create loading states and error handling

## Deployment ðŸš§

### 10.1 Prepare for Deployment

- [x] Secure API keys (store them in environment variables or a secure configuration file)
- [x] Update README with clear instructions on running the project
- [ ] (Optional) Containerize the application with Docker

### 10.2 Choose a Deployment Option

- [ ] Deploy on a cloud provider (use a small VM or a serverless platform)
- [ ] Monitor usage and cost, especially for OpenAI API calls and Pinecone usage

## Final Checklist ðŸš§

- [x] Environment setup completed (Python installed, virtual environment created, dependencies installed)
- [x] Project structure created with all required directories and files
- [x] CSV data is successfully loaded using the ingestion pipeline
- [x] External data fetching functions are implemented and tested
- [x] Text chunking function is implemented and returns correct chunks
- [x] Embedding generation function works correctly using OpenAI's API
- [x] Pinecone index (or your chosen vector DB) is initialized and tested
- [x] Data is successfully upserted into the vector database
- [x] Query embedding and retrieval functions are working as expected
- [x] Prompt construction correctly integrates user queries and retrieved context
- [x] GPT-3.5-Turbo integration returns a valid response
- [ ] End-to-end tests pass without errors
- [ ] Deployment instructions are documented and ready for production

## Implementation Plan

### Completed Features âœ…

1. **Core API Structure**
   - [x] Set up FastAPI application with proper routing
   - [x] Implement error handling and response models
   - [x] Add request validation and rate limiting
   - [x] Set up CORS and middleware

2. **Data Models and Validation**
   - [x] Define Pydantic models for requests and responses
   - [x] Implement input validation
   - [x] Create response schemas
   - [x] Add proper error models

3. **Vector Search Integration**
   - [x] Implement core vector search functionality in `vector_db.py`
   - [x] Add Pinecone initialization and connection handling
   - [x] Create embedding generation pipeline in `embedding.py`
   - [x] Implement restaurant search with filters
   - [x] Add vector cleanup and maintenance functions
   - [x] Write comprehensive tests for vector search functionality
   - [x] Add proper error handling and retries for API calls
   - [x] Implement batch processing for embeddings

4. **Restaurant Search API**
   - [x] Implement semantic search for restaurants
   - [x] Add filtering by price range and rating
   - [x] Implement pagination for search results
   - [x] Add sorting by relevance score
   - [x] Create detailed restaurant information endpoint

### Next Steps ðŸ”„

1. **Chat Feature Implementation**
   - [x] Enhance chat context management
   - [x] Improve conversation history handling
   - [x] Add chat session persistence
   - [x] Implement context-aware responses
   - [x] Add conversation metadata tracking
   - [x] Add conversation cleanup functionality
   - [x] Add conversation retrieval endpoints
   - [ ] Add rate limiting for chat endpoints
   - [ ] Add user authentication and authorization

2. **Testing and Documentation**
   - [x] Add unit tests for conversation management
   - [ ] Add integration tests for chat endpoints
   - [ ] Create API documentation with examples
   - [ ] Add performance benchmarks
   - [ ] Document deployment process

3. **Frontend Development** ðŸ”„
   - [ ] Design and implement search interface
   - [ ] Create chat UI components
   - [ ] Add restaurant detail views
   - [ ] Implement responsive design

4. **Deployment and Infrastructure** ðŸ”„
   - [ ] Set up CI/CD pipeline
   - [ ] Configure production environment
   - [ ] Implement monitoring and logging
   - [ ] Add performance optimization

5. **Authentication and Authorization** ðŸ”„
   - [ ] Implement user authentication system
   - [ ] Add role-based access control
   - [ ] Set up secure session management
   - [ ] Add API key management

6. **Performance Optimization** ðŸ”„
   - [ ] Implement caching for frequent queries
   - [ ] Optimize database queries
   - [ ] Add request batching
   - [ ] Implement connection pooling

### Latest Progress ðŸ“ˆ

1. **Testing Infrastructure (Completed)** âœ…
   - Added comprehensive end-to-end test suite
   - Implemented mock fixtures for all external dependencies
   - Added proper test isolation with temporary directories
   - Added conversation persistence testing
   - Added cleanup testing
   - Added error scenario testing
   - Added context window handling tests
   - All tests passing successfully

2. **Rate Limiting (Completed)** âœ…
   - Implemented rate limiting for all endpoints
   - Added configurable limits per endpoint
   - Added proper error responses for rate limits
   - Added rate limit headers
   - Added rate limit bypass for testing
   - Added comprehensive rate limit tests

3. **Error Handling (Completed)** âœ…
   - Added comprehensive error handling throughout
   - Implemented proper HTTP error responses
   - Added detailed error messages
   - Added error logging
   - Added error recovery mechanisms
   - Added error scenario tests

4. **Code Quality (Completed)** âœ…
   - Added type hints throughout
   - Added comprehensive documentation
   - Improved code organization
   - Added proper logging
   - Added proper configuration management
   - Added proper dependency injection

### Next Steps ðŸŽ¯

1. **Frontend Development**
   - [ ] Set up React/Next.js project structure
   - [ ] Create component library
   - [ ] Implement API integration
   - [ ] Add state management

2. **Deployment**
   - [ ] Set up Docker containerization
   - [ ] Configure cloud infrastructure
   - [ ] Set up monitoring
   - [ ] Configure logging

3. **Authentication**
   - [ ] Design auth system
   - [ ] Implement user management
   - [ ] Add session handling
   - [ ] Set up secure storage

